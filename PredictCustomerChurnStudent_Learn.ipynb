{"nbformat_minor": 1, "cells": [{"cell_type": "markdown", "metadata": {}, "source": "## Predict Customer Churn Use Case Implementation\nThe objective is to follow the CRISP-DM methodology to build a model to predict customer churn\n![CRISP-DM](https://raw.githubusercontent.com/yfphoon/dsx_demo/master/crisp_dm.png)\n\n\n## Table of contents\n\n1. [Step 1: Download the customer churn data](#download)<br/>\n2. [Step 2: Read data into Spark DataFrames](#getdata)<br/>\n3. [Step 3: Merge Files](#merge)<br/>\n4. [Step 4: Rename some columns](#rename)<br/>\n5. [Step 5: Data understanding](#dataunderstanding)<br/>\n    5.1 [Dataset overview](#overview)<br/>\n    5.2 [Exploratory data analysis](#eda)<br/>\n    5.3 [Interactive query with SparkSQL](#sparksql)<br/>\n6. [Step 6: Introduction to Spark pipelines](#intropipeline)<br/>\n    6.1 [StringIndexer](#stringindexer)<br/>\n    6.2 [IndexToString](#indextostring)<br/>\n    6.3 [OneHotEncoder](#onehotencoder)<br/>\n    6.4 [Bucketizer](#bucketizer)<br/>\n    6.4 [VectorAssembler](#vectorassembler)<br/>\n    6.4 [Normalizer](#normalizer)<br/>\n7. [Step 7: Applying Spark pipeline concepts to customer churn data](#applypipelineconcepts)<br/>\n8. [Step 8: Creating a Spark ML pipeline](#createpipeline)<br/>\n9. [Step 9: Score the test dataset](#scoretestdata)<br/>\n10. [Step 10: Model evaluation](#evaluate)<br/>\n11. [Step 11: Tune the hyperparameters to find the best model](#tune)<br/>\n12. [Step 12: Execute inline invocation of best model](#execute)<br/>\n13. [Step 13: Save model](#save)<br/>"}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"download\"></a>\n# <span style=\"color:#fa04d9\"> Step 1: Download the customer churn data</span>"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "#Run once to install the wget package\n!pip install wget", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {}, "source": "# download data from GitHub repository\nimport wget\nurl_churn='https://raw.githubusercontent.com/SidneyPhoon/IntroToNotebooksLab/master/data/churn.csv'\nurl_customer='https://raw.githubusercontent.com/SidneyPhoon/IntroToNotebooksLab/master/data/customer.csv'\n\n#remove existing files before downloading\n!rm -f churn.csv\n!rm -f customer.csv\n\nchurnFilename=wget.download(url_churn)\ncustomerFilename=wget.download(url_customer)\n\n#list existing files\n!ls -l churn.csv\n!ls -l customer.csv", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"getdata\"></a>\n# <span style=\"color:#fa04d9\">Step 2: Read data into Spark DataFrames</span>\n\nNote: You want to reference the Spark DataFrame API to learn more about the supported operations, https://spark.apache.org/docs/2.0.0-preview/api/python/pyspark.sql.html#pyspark.sql.DataFrame"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\nchurn_df= spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .option(\"inferSchema\", \"true\")\\\n  .load(\"churn.csv\")\n\ncustomer_df = sqlContext.read\\\n    .format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\")\\\n    .option(\"header\", \"true\")\\\n    .option(\"inferSchema\", \"true\")\\\n    .load(\"customer.csv\")", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "#### <span style=\"color:blue\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Take a look at the 5 first datapoints from the newly loaded Spark dataframes.</span>"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "customer_df.show(5)", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# show the schema\ncustomer_df.printSchema()", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "churn_df.show(5)", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "churn_df.printSchema()", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"merge\"></a>\n# <span style=\"color:#fa04d9\">Step 3: Merge Files </span>\n"}, {"outputs": [], "cell_type": "code", "metadata": {"scrolled": false, "collapsed": true}, "source": "data_df = customer_df.join(churn_df,customer_df['ID'] == churn_df['ID']).select(customer_df['*'], churn_df['CHURN'])", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "data_df.show(5)", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"rename\"></a>\n# <span style=\"color:#fa04d9\">Step 4: Rename some columns </span>\nThis step is not a requirement, it just makes some columns names simpler to type with no spaces"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# withColumnRenamed renames an existing column in a SparkDataFrame and returns a new SparkDataFrame\ndata_df = data_df.withColumnRenamed(\"Est Income\", \"EstIncome\").withColumnRenamed(\"Car Owner\",\"CarOwner\")\ndata_df.toPandas().head()", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"dataunderstanding\"></a>\n# <span style=\"color:#fa04d9\">Step 5: Data understanding </span>"}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"overview\"></a>\n### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Dataset Overview\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The Pandas library has a powerful set commands to analyze data. As an example, check the use of \"describe\" below."}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "df_pandas = data_df.toPandas()\nprint \"There are \" + str(len(df_pandas)) + \" observations in the customer history dataset.\"\nprint \"There are \" + str(len(df_pandas.columns)) + \" variables in the dataset.\"\n\nprint \"\\n******************Descriptive statistics*****************************\\n\"\nprint df_pandas.drop(['ID'], axis = 1).describe()\n", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"eda\"></a>\n### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Exploratory Data Analysis"}, {"cell_type": "markdown", "metadata": {}, "source": "The **Brunel** Visualization library provides a highly succinct and novel language that defines interactive data visualizations based on tabular data. The language is well suited for both data scientists and more aggressive business users. The system interprets the language and produces visualizations using the user's choice of existing lower-level visualization technologies typically used by application engineers such as RAVE or D3. \n\nMore information about Brunel Visualization: https://github.com/Brunel-Visualization/Brunel/wiki\n\nTry Brunel visualization here:  http://brunel.mybluemix.net/gallery_app/renderer"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "import brunel\ndf_pandas = data_df.toPandas()\n%brunel data('df_pandas') stack bar x(Paymethod) y(#count) color(CHURN) bin(Paymethod) percent(#count) label(#count) tooltip(#all) | x(LongDistance) y(Usage) point color(Paymethod) tooltip(LongDistance, Usage) :: width=1100, height=400 ", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# Heat map\n%brunel data('df_pandas') x(LocalBilltype) y(Dropped) color(#count:red) style('symbol:rect; size:100%; stroke:none') tooltip(Dropped,#count)", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "**PixieDust** is a Python Helper library for Spark IPython Notebooks. One of it's main features are visualizations. You'll notice that unlike other APIs which produce just output, PixieDust creates an interactive UI in which you can explore data.<br/>\nMore information about PixieDust: https://github.com/ibm-cds-labs/pixiedust?cm_mc_uid=78151411419314871783930&cm_mc_sid_50200000=1487962969"}, {"cell_type": "markdown", "metadata": {}, "source": "**If you haven't already installed it, uncomment and run the following cell to install the pixiedust Python library in your notebook environment. You only need to run it once**\n"}, {"outputs": [], "cell_type": "code", "metadata": {"scrolled": true, "collapsed": true}, "source": "#!pip install --user --upgrade pixiedust", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"pixiedust": {"displayParams": {"valueFields": "Usage", "aggregation": "AVG", "keyFields": "Paymethod", "rowCount": "500", "handlerId": "barChart", "chartsize": "51"}}, "collapsed": true}, "source": "from pixiedust.display import *\ndisplay(data_df)", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"sparksql\"></a>\n### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Interactive query with Spark SQL"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# Spark SQL also allow you to use standard SQL\ndata_df.createOrReplaceTempView(\"data_df\")\nsql = \"\"\"\nSELECT c.*\nFROM data_df c\nWHERE c.EstIncome>90000\n\n\"\"\"\nspark.sql(sql).toPandas().head()", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"intropipeline\"></a>\n# <span style=\"color:#fa04d9\">Step 6: Introduction to Spark Pipelines (Optional. if you are already familiar with these concepts, please skip to Step 7).</span>"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, IndexToString\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### In the section following this one, you will be building a SparkML Pipeline which consists of Transformers and Estimators. As a preamble to that section, users who are not familiar with the concepts and terminology of \"Transformers\", \"Estimators\" and \"Pipeline\" are invited to take advantage of this section to get familiarity with those concepts. Users who are already familiar with these concepts can skip directly to the next section of this notebook: Step 7"}, {"cell_type": "markdown", "metadata": {}, "source": "\n## <span style=\"color:green\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In this section, you will get familiar with a few important Spark ML concepts:\n### <span style=\"color:green\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Discovering some Estimators, Transformers and what they do.\n### <span style=\"color:green\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Introduction to the notion of a Spark Machine Learning Pipeline.</span>"}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"stringindexer\"></a>\n## <span style=\"color:green\">Getting familiar with the SparkML Estimator: <a href=\"https://spark.apache.org/docs/latest/ml-features.html#stringindexer\">StringIndexer</a> </span>"}, {"cell_type": "markdown", "metadata": {}, "source": "### StringIndexer encodes a string column of labels to a column of label indices. The indices are in [0, numLabels), ordered by label frequencies, so the most frequent label gets index 0.<br><br> Note that StringIndexer is an estimator, not a transformer. StringIndexer needs to scan the data it is given as input, to find the most frequent string and assign to it label 0, and then label 1 to the next most frequent string and so on. It will then produce a StringIndexerModel which is a transformer which can be applied to the input data using the \"transform\" method."}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"panel-group\" id=\"accordion-10\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-10\" href=\"#collapse1-10\">\n        Click on this link to expand this cell, then copy and paste the code which will appear in a new cell just below, and execute that new cell to see how StringIndexer works. (You may subsequently delete that new cell and proceed with this notebook).</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-10\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">\nfrom pyspark.ml.feature import StringIndexer<br>\n<br>\ndf = spark.createDataFrame( <br>\n    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")], <br>\n    [\"id\", \"category\"]) <br>\n<br>\nindexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\") <br>\nindexed = indexer.fit(df).transform(df) <br>\nindexed.show()\n      </div>\n    </div>\n  </div>"}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"indextostring\"></a>\n## <span style=\"color:green\">Getting familiar with the SparkML Transformer: <a href=\"https://spark.apache.org/docs/latest/ml-features.html#indextostring\">IndexToString</a> </span>"}, {"cell_type": "markdown", "metadata": {}, "source": "### Symmetrically to StringIndexer, IndexToString maps a column of label indices back to a column containing the original labels as strings. A common use case is to produce indices from labels with StringIndexer, train a model with those indices and retrieve the original labels from the column of predicted indices with IndexToString. However, you are free to supply your own labels."}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"panel-group\" id=\"accordion-11\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-11\" href=\"#collapse1-11\">\n        Click on this link to expand this cell, then copy and paste the code which will appear in a new cell just below, and execute that new cell to see how IndexToString works. (You may subsequently delete that new cell and proceed with this notebook).</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-11\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">\nfrom pyspark.ml.feature import IndexToString, StringIndexer <br>\n<br>\ndf = spark.createDataFrame(<br>\n    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],<br>\n    [\"id\", \"category\"])<br>\n<br>\nindexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")<br>\nmodel = indexer.fit(df)<br>\nindexed = model.transform(df)<br>\n<br>\nprint(\"Transformed string column '%s' to indexed column '%s'\"<br>\n      % (indexer.getInputCol(), indexer.getOutputCol()))<br>\nindexed.show()<br>\n<br>\nprint(\"StringIndexer will store labels in output column metadata\\n\")<br>\n<br>\nconverter = IndexToString(inputCol=\"categoryIndex\", outputCol=\"originalCategory\")<br>\nconverted = converter.transform(indexed)<br>\n<br>\nprint(\"Transformed indexed column '%s' back to original string column '%s' using \"<br>\n      \"labels in metadata\" % (converter.getInputCol(), converter.getOutputCol()))<br>\nconverted.select(\"id\", \"categoryIndex\", \"originalCategory\").show()\n      </div>\n    </div>\n  </div>"}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"onehotencoder\"></a>\n## <span style=\"color:green\">Getting familiar with the SparkML Transformer: <a href=\"https://spark.apache.org/docs/latest/ml-features.html#onehotencoder\">OneHotEncoder</a> </span>"}, {"cell_type": "markdown", "metadata": {}, "source": "### One-hot encoding maps a column of label indices to a column of binary vectors, with at most a single one-value. This encoding allows algorithms which expect continuous (quantitative to be precise as the output is discrete) features, such as Logistic Regression, to use categorical features. OneHotEncoder is a transformer."}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"panel-group\" id=\"accordion-12\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-12\" href=\"#collapse1-12\">\n        Click on this link to expand this cell, then copy and paste the code which will appear in a new cell just below, and execute that cell to see how OneHotEncoder works. (You may subsequently delete that new cell and proceed with this notebook).</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-12\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">\n          df = spark.createDataFrame([ <br>\n    (0, \"a\"), <br>\n    (1, \"b\"), <br>\n    (2, \"c\"), <br>\n    (3, \"a\"), <br>\n    (4, \"a\"), <br>\n    (5, \"c\")  <br>\n    ], [\"id\", \"category\"]) <br>\n<br>\nstringIndexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\") <br>\nmodel = stringIndexer.fit(df) <br>\nindexed = model.transform(df) <br>\n<br>\nencoder = OneHotEncoder(inputCol=\"categoryIndex\", outputCol=\"categoryVec\") <br>\nencoded = encoder.transform(indexed) <br>\nencoded.show()\n      </div>\n    </div>\n  </div>"}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"bucketizer\"></a>\n## <span style=\"color:green\">Getting familiar with the SparkML Transformer: <a href=\"https://spark.apache.org/docs/latest/ml-features.html#bucketizer\">Bucketizer</a> </span>"}, {"cell_type": "markdown", "metadata": {}, "source": "### Bucketizer transforms a column of continuous features to a column of feature buckets, where the buckets are specified by users. It takes a parameter defining the number of buckets. Bucketizing data is also referred to as \"binning\"."}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"panel-group\" id=\"accordion-13\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-13\" href=\"#collapse1-13\">\n        Click on this link to expand this cell, then copy and paste the code which will appear in a new cell just below, and execute that cell to see how Bucketizer works. (You may subsequently delete that new cell and proceed with this notebook).</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-13\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">\nfrom pyspark.ml.feature import Bucketizer<br>\n<br>\nsplits = [-float(\"inf\"), -0.5, 0.0, 0.5, float(\"inf\")] <br>\n<br>\ndata = [(-999.9,), (-0.5,), (-0.3,), (0.0,), (0.2,), (999.9,)] <br>\ndataFrame = spark.createDataFrame(data, [\"features\"]) <br>\n<br>\nbucketizer = Bucketizer(splits=splits, inputCol=\"features\", outputCol=\"bucketedFeatures\") <br>\n<br>\n# Transform original data into its bucket index. <br>\nbucketedData = bucketizer.transform(dataFrame) <br>\n<br>\nprint(\"Bucketizer output with %d buckets\" % (len(bucketizer.getSplits())-1)) <br>\nbucketedData.show()\n      </div>\n    </div>\n  </div>"}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"vectorassembler\"></a>\n## <span style=\"color:green\">Getting familiar with the SparkML Transformer: <a href=\"https://spark.apache.org/docs/latest/ml-features.html#vectorassembler\">VectorAssembler</a> </span>"}, {"cell_type": "markdown", "metadata": {}, "source": "### VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees."}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"panel-group\" id=\"accordion-14\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-14\" href=\"#collapse1-14\">\n        Click on this link to expand this cell, then copy and paste the code which will appear in a new cell just below, and execute that cell to see how VectorAssembler works. (You may subsequently delete that new cell and proceed with this notebook).</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-14\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">\nfrom pyspark.ml.linalg import Vectors <br>\nfrom pyspark.ml.feature import VectorAssembler <br>\n<br>\ndataset = spark.createDataFrame( <br>\n    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)], <br>\n    [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"]) <br>\n<br>\nassembler = VectorAssembler( <br>\n    inputCols=[\"hour\", \"mobile\", \"userFeatures\"], <br>\n    outputCol=\"features\") <br>\n<br>\noutput = assembler.transform(dataset) <br>\nprint(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\") <br>\noutput.select(\"features\", \"clicked\").show(truncate=False) <br>\n      </div>\n    </div>\n  </div>"}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"normalizer\"></a>\n## <span style=\"color:green\">Getting familiar with the SparkML Transformer: <a href=\"https://spark.apache.org/docs/latest/ml-features.html#normalizer\">Normalizer</a> </span>"}, {"cell_type": "markdown", "metadata": {}, "source": "### Normalizer is a Transformer which transforms a dataset of Vector rows, normalizing each Vector to have unit norm. It takes parameter p, which specifies the p-norm used for normalization. (p=2 by default.) This normalization can help standardize your input data and improve the behavior of learning algorithms."}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"panel-group\" id=\"accordion-15\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-15\" href=\"#collapse1-15\">\n        Click on this link to expand this cell, then copy and paste the code which will appear in a new cell just below, and execute that cell to see how Normalizer works. (You may subsequently delete that new cell and proceed with this notebook).</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-15\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">\nfrom pyspark.ml.feature import Normalizer<br>\nfrom pyspark.ml.linalg import Vectors<br>\n<br>\ndataFrame = spark.createDataFrame([ <br>\n    (0, Vectors.dense([1.0, 0.5, -1.0]),), <br>\n    (1, Vectors.dense([2.0, 1.0, 1.0]),), <br>\n    (2, Vectors.dense([4.0, 10.0, 2.0]),) <br>\n], [\"id\", \"features\"]) <br>\n<br>\n# Normalize each Vector using $L^1$ norm.<br>\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)<br>\nl1NormData = normalizer.transform(dataFrame) <br>\nprint(\"Normalized using L^1 norm\") <br>\nl1NormData.show() <br>\n<br>\n# Normalize each Vector using $L^\\infty$ norm. <br>\nlInfNormData = normalizer.transform(dataFrame, {normalizer.p: float(\"inf\")}) <br>\nprint(\"Normalized using L^inf norm\") <br>\nlInfNormData.show()\n      </div>\n    </div>\n  </div>"}, {"cell_type": "markdown", "metadata": {}, "source": "## <span style=\"color:green\">There are several other Estimators and Transformers which are documented in the Apache documentation online right <a href=\"https://spark.apache.org/docs/latest/ml-features.html\">here</a>"}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"applypipelineconcepts\"></a>\n# <span style=\"color:#fa04d9\">Step 7: Applying the concepts described above to our customer churn dataset: \n** * a) Defining and applying the StringIndexer Estimator to input columns Gender, Status, CarOwner, Paymethod, LocalBilltype, LongDistanceBilltype. **<br>\n** * b) Defining and applying VectorAssembler to the columns above to group them as one input vector to the model. **<br>\n** * c) Defining and applying a StringIndexer Estimator to the target label column \"CHURN\", to encode the T/F values into 0/1. ** <br>\n** * d) Defining and applying an IndexToString Transformer to reverse the output of our model from 0/1 predictions back to T/F values . ** <br>\n** * e) Defining the Random Forest estimator itself, which will be trained on the input training data to produce the actual model which will perform the predictions. **"}, {"cell_type": "markdown", "metadata": {}, "source": "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a) Defining a StringIndexer for the String columns in our dataset"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "### In this dataset, we will encode columns Gender, Status, CarOwner, Paymethod, LocalBilltype and LongDistanceBilltype\n# StringIndexer encodes a string column of labels to a column of label indices. \nSI1 = StringIndexer(inputCol='Gender', outputCol='GenderEncoded')\nSI2 = StringIndexer(inputCol='Status',outputCol='StatusEncoded')\nSI3 = StringIndexer(inputCol='CarOwner',outputCol='CarOwnerEncoded')\nSI4 = StringIndexer(inputCol='Paymethod',outputCol='PaymethodEncoded')\nSI5 = StringIndexer(inputCol='LocalBilltype',outputCol='LocalBilltypeEncoded')\nSI6 = StringIndexer(inputCol='LongDistanceBilltype',outputCol='LongDistanceBilltypeEncoded')", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b) Define a Vector Assembler for all the columns of interest to be passed into the chosen machine learning model (columns which are encoded as well as those kept as is)"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# Pipelines API requires that input variables are passed in  a vector\nassembler = VectorAssembler(inputCols=[\"GenderEncoded\", \"StatusEncoded\", \"CarOwnerEncoded\", \"PaymethodEncoded\", \\\n                                       \"LocalBilltypeEncoded\", \"LongDistanceBilltypeEncoded\", \"Children\", \"EstIncome\", \"Age\", \\\n                                       \"LongDistance\", \"International\", \"Local\", \"Dropped\",\"Usage\",\"RatePlan\"], outputCol=\"features\")", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;c) Defining a StringIndexer for the label column of our model (CHURN column. The values True and False will be converted to 0 and 1)"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# encode the label column\nlabelIndexer = StringIndexer(inputCol='CHURN', outputCol='label').fit(data_df)", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;d) Defining an IndexToString transformer to bring the labels back to True and False once the predictions are done. The model will produce a column named \"prediction\" which will contain 0 or 1. We will convert it back to True and False in a column named \"predictedLabel\""}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# Convert indexed labels back to original labels.\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;e) Defining a Random Forest estimator. This is a popular tree based classifier method"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# instantiate the algorithm, take the default settings\nrf=RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"createpipeline\"></a>\n# <span style=\"color:#fa04d9\">Step 8: Creating a Spark ML pipeline:\n** * All the individual components of the pipeline have been defined in the section above. Notice how we will now \"group\" them into a pipeline object</span> **"}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": "### In machine learning, it is common to run a sequence of algorithms to process and learn from data. E.g., a simple text document processing workflow might include several stages:\n* Split each document\u2019s text into words. \n* Convert each document\u2019s words into a numerical feature vector.  \n* Learn a prediction model using the feature vectors and labels.<br>\n\n### MLlib represents such a workflow as a Pipeline, which consists of a sequence of PipelineStages (Transformers and Estimators) to be run in a specific order. "}, {"cell_type": "markdown", "metadata": {}, "source": "We will now build the Spark pipeline including the operations defined in Step 7 above.\n\"Pipeline\" is an API in SparkML. A pipeline defines a sequence of transformers and estimators to perform the analysis in stages.\nAdditional information on SparkML is available online, including at this link: https://spark.apache.org/docs/2.0.2/ml-guide.html"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# build the pipeline\npipeline = Pipeline(stages=[SI1,SI2,SI3,SI4,SI5,SI6, labelIndexer, assembler, rf, labelConverter])", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### Split the data into Training and Testing sets (this is a standard best practice in data science)"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# Split data into train and test datasets\n(trainingData, testingData) = data_df.randomSplit([0.7, 0.3],seed=9)\ntrainingData.cache()\ntestingData.cache()", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### Build the model from fitting the whole pipeline using the training data set. <br><br>Note that the pipeline interface will correctly call fit+transform or just transform alone for each stage of the pipeline, depending on whether the current stage is an estimator (such as StringIndex) or a Transformer"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# Build model. The fitted model from a Pipeline is a PipelineModel, which consists of fitted models and transformers, corresponding to the pipeline stages.\nmodel = pipeline.fit(trainingData)", "execution_count": null}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": "<a id=\"scoretestdata\"></a>\n# <span style=\"color:#fa04d9\">Step 9: Score the test data set </span>"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "result=model.transform(testingData)\nresult_display=result.select(result[\"ID\"],result[\"CHURN\"],result[\"Label\"],result[\"predictedLabel\"],result[\"prediction\"],result[\"probability\"])\nresult_display.toPandas().head(6)", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"evaluate\"></a>\n# <span style=\"color:#fa04d9\">Step 10: Model Evaluation </span>\n** Find accuracy of the model and the Area Under the ROC Curve **"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "print 'Model Accuracy = {:.2f}.'.format(result.filter(result.label == result.prediction).count() / float(result.count()))", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### Create an evaluator for the binary classification using area under the ROC Curve as the evaluation metric\nReceiver operating characteristic (ROC) is a graphical plot that illustrates the performance of a binary classifier system as its discrimination threshold is varied.\n\nAdditional reading on this metric can be found very easily online, such as at this wikipedia link: https://en.wikipedia.org/wiki/Receiver_operating_characteristic\n"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\nprint 'Area under ROC curve = {:.2f}.'.format(evaluator.evaluate(result))", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"tune\"></a>\n# <span style=\"color:#fa04d9\">Step 11:  Tune the hyperparameters to find the best model </span>"}, {"cell_type": "markdown", "metadata": {}, "source": "### Build a Parameter Grid specifying the parameters to be evaluated to determine the best combination"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# set different levels for the maxDepth\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nparamGrid = (ParamGridBuilder().addGrid(rf.maxDepth,[4,6,8]).build())", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### Create a cross validator to tune the pipeline with the generated parameter grid\nCross-validation attempts to fit the underlying estimator with user-specified combinations of parameters, cross-evaluate the fitted models, and output the best one."}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# perform 3 fold cross validation\ncv = CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# train the model\ncvModel = cv.fit(trainingData)\n\n# pick the best model\nbest_rfModel = cvModel.bestModel", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# score the test data set with the best model\ncvresult=best_rfModel.transform(testingData)\ncvresults_show=cvresult.select(cvresult[\"ID\"],cvresult[\"CHURN\"],cvresult[\"Label\"],cvresult[\"predictedLabel\"],cvresult[\"prediction\"],cvresult[\"probability\"])\ncvresults_show.toPandas().head()", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "print 'Model Accuracy of the best fitted model = {:.2f}.'.format(cvresult.filter(cvresult.label == cvresult.prediction).count()/ float(cvresult.count()))\nprint 'Model Accuracy of the default model = {:.2f}.'.format(result.filter(result.label == result.prediction).count() / float(result.count()))\nprint '   '\nprint('Area under the ROC curve of best fitted model = {:.2f}.'.format(evaluator.evaluate(cvresult)))\nprint 'Area under the ROC curve of the default model = {:.2f}.'.format(evaluator.evaluate(result))", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"execute\"></a>\n# <span style=\"color:#fa04d9\">Step 12: Execute an inline invocation of the best model which was just identified </span>"}, {"cell_type": "markdown", "metadata": {}, "source": "### Let us now make a prediction on some customer for which we will provide our own made up attributes"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "Gender = 'F'\nStatus = 'M'\nCarOwner = 'N'\nPaymethod = 'CC'\nLocalBilltype = 'Budget'\nLongDistanceBilltype = 'Standard'\nChildren = 1\nEstIncome = 45000\nAge = 30\nLongDistance = 30\nInternational = 0\nLocal = 100\nDropped = 0\nUsage = 150\nRatePlan = 2\n\nFeatures = (spark.createDataFrame([(Gender, Status, CarOwner, Paymethod, LocalBilltype, LongDistanceBilltype, Children, EstIncome, Age, LongDistance, \\\n                                              International, Local, Dropped, Usage, RatePlan)],\n    ['Gender', 'Status', 'CarOwner', 'Paymethod', 'LocalBilltype', 'LongDistanceBilltype', 'Children', 'EstIncome', 'Age', 'LongDistance', \\\n     'International', 'Local', 'Dropped', 'Usage', 'RatePlan']))\nFeatures.show()", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "ChurnPrediction = best_rfModel.transform(Features)\nChurnPrediction.select('rawPrediction', 'probability', 'prediction').show(1, False)", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### Mini Exercise: Change the number of children and/or the EstIncome in the cell prior to the one above, and observe the impact on the prediction:\n* It seems that a number of children lower than 3 will result in churn, but a customer with 3 children or more will not churn.\n* The rule above is true for lower incomes. With higher incomes, churn is less likely (if we change the income to 145,000 the model does not seem to predict churn anymore, regardless of the number of children)"}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"save\"></a>\n# <span style=\"color:#fa04d9\"> Step 13: Save Model </span>\n** Save the best model in Object Storage. **\n\n** A separate notebook has been created for \"batch scoring deployment\". This deployment notebook retrieves the model from object storage and applies it to a new dataset. The notebook can be scheduled to run via the Notebook scheduler or through the deployment interface in IBM WML. (In order to schedule through WML, the model needs to first be saved in the WML repository, which can be done using the appropriate API calls) **"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# Overwrite any existing saved model in the specified path\nbest_rfModel.write().overwrite().save(\"PredictChurn.churnModel\")", "execution_count": null}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": "You have come to the end of this notebook"}, {"cell_type": "markdown", "metadata": {}, "source": "**Sidney Phoon**<br/>\n**Elena Lowery**<br/>\n**Rich Tarro**<br/>\n**Mokhtar Kandil**<br/>\nJuly, 2017"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "", "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 2 with Spark 2.0", "name": "python2-spark20", "language": "python"}, "language_info": {"mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 2}, "pygments_lexer": "ipython2", "version": "2.7.11", "name": "python", "file_extension": ".py", "nbconvert_exporter": "python"}}, "nbformat": 4}