{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is a streamlined version of the \"PredictCustomerChurnStudent_Learn\" version which is a prerequisite to this one. If you have not done so, please check first (and execute) the student learning version of the notebook.\n",
    "\n",
    "# How to use this notebook:\n",
    "# * Click on \"Cell ==> RunAll\".\n",
    "# * Scroll down to the bottom of the notebook (usually done automatically) and wait for the last cell to execute.\n",
    "# * Complete <span style=\"color:#fa04d9\">Steps 14 and 15 </span> which are the optional practice exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================================================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#fa04d9\"> Step 1: Download the customer churn data</span>\n",
    "** This should have been done by the learning version of this notebook which is a prerequisite to this one.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#fa04d9\">Step 2: Read data into Spark DataFrames</span>\n",
    "\n",
    "Note: You want to reference the Spark DataFrame API to learn more about the supported operations, https://spark.apache.org/docs/2.0.0-preview/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "churn_df = sqlContext.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"churn.csv\")\n",
    "customer_df = sqlContext.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"customer.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#fa04d9\">Step 3: Merge Files </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_df = customer_df.join(churn_df,customer_df['ID'] == churn_df['ID']).select(customer_df['*'], churn_df['CHURN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#fa04d9\">Step 4: Rename some columns </span>\n",
    "This step is not a requirement, it just makes some columns names simpler to type with no spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# withColumnRenamed renames an existing column in a SparkDataFrame and returns a new SparkDataFrame\n",
    "data_df = data_df.withColumnRenamed(\"Est Income\", \"EstIncome\").withColumnRenamed(\"Car Owner\",\"CarOwner\")\n",
    "data_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#fa04d9\">Step 5: Data understanding (Not required for this version of the notebook. Please refer to the Learning version for this section) </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#fa04d9\">Step 6: Introduction to Spark Pipelines (Optional. if you are already familiar with these concepts, please skip to Step 7).</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, IndexToString\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the section following this one, you will be building a SparkML Pipeline which consists of Transformers and Estimators. As a preamble to that section, users who are not familiar with the concepts and terminology of \"Transformers\", \"Estimators\" and \"Pipeline\" are invited to take advantage of this section to get familiarity with those concepts. Users who are already familiar with these concepts can skip directly to the next section of this notebook: Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <span style=\"color:green\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In this section, you will get familiar with a few important Spark ML concepts:\n",
    "### <span style=\"color:green\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Discovering some Estimators, Transformers and what they do.\n",
    "### <span style=\"color:green\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Introduction to the notion of a Spark Machine Learning Pipeline.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">Getting familiar with the SparkML Estimator: StringIndexer </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StringIndexer encodes a string column of labels to a column of label indices. The indices are in [0, numLabels), ordered by label frequencies, so the most frequent label gets index 0.<br><br> Note that StringIndexer is an estimator, not a transformer. StringIndexer needs to scan the data it is given as input, to find the most frequent string and assign to it label 0, and then label 1 to the next most frequent string and so on. It will then produce a StringIndexerModel which is a transformer which can be applied to the input data using the \"transform\" method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel-group\" id=\"accordion-10\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-10\" href=\"#collapse1-10\">\n",
    "        Click on this link to expand this cell, then copy and paste the code which will appear in a new cell just below, and execute that new cell to see how StringIndexer works. (You may subsequently delete that new cell and proceed with this notebook).</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-10\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "from pyspark.ml.feature import StringIndexer<br>\n",
    "<br>\n",
    "df = spark.createDataFrame( <br>\n",
    "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")], <br>\n",
    "    [\"id\", \"category\"]) <br>\n",
    "<br>\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\") <br>\n",
    "indexed = indexer.fit(df).transform(df) <br>\n",
    "indexed.show()\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">Getting familiar with the SparkML Transformer: IndexToString </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetrically to StringIndexer, IndexToString maps a column of label indices back to a column containing the original labels as strings. A common use case is to produce indices from labels with StringIndexer, train a model with those indices and retrieve the original labels from the column of predicted indices with IndexToString. However, you are free to supply your own labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel-group\" id=\"accordion-11\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-11\" href=\"#collapse1-11\">\n",
    "        Click on this link to expand this cell, then copy and paste the code which will appear in a new cell just below, and execute that new cell to see how IndexToString works. (You may subsequently delete that new cell and proceed with this notebook).</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-11\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer <br>\n",
    "<br>\n",
    "df = spark.createDataFrame(<br>\n",
    "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],<br>\n",
    "    [\"id\", \"category\"])<br>\n",
    "<br>\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")<br>\n",
    "model = indexer.fit(df)<br>\n",
    "indexed = model.transform(df)<br>\n",
    "<br>\n",
    "print(\"Transformed string column '%s' to indexed column '%s'\"<br>\n",
    "      % (indexer.getInputCol(), indexer.getOutputCol()))<br>\n",
    "indexed.show()<br>\n",
    "<br>\n",
    "print(\"StringIndexer will store labels in output column metadata\\n\")<br>\n",
    "<br>\n",
    "converter = IndexToString(inputCol=\"categoryIndex\", outputCol=\"originalCategory\")<br>\n",
    "converted = converter.transform(indexed)<br>\n",
    "<br>\n",
    "print(\"Transformed indexed column '%s' back to original string column '%s' using \"<br>\n",
    "      \"labels in metadata\" % (converter.getInputCol(), converter.getOutputCol()))<br>\n",
    "converted.select(\"id\", \"categoryIndex\", \"originalCategory\").show()\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">Getting familiar with the SparkML Transformer: OneHotEncoder </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding maps a column of label indices to a column of binary vectors, with at most a single one-value. This encoding allows algorithms which expect continuous (quantitative to be precise as the output is discrete) features, such as Logistic Regression, to use categorical features. OneHotEncoder is a transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel-group\" id=\"accordion-12\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-12\" href=\"#collapse1-12\">\n",
    "        Click on this link to expand this cell, then copy and paste the code which will appear in a new cell just below, and execute that cell to see how OneHotEncoder works. (You may subsequently delete that new cell and proceed with this notebook).</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-12\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "          df = spark.createDataFrame([ <br>\n",
    "    (0, \"a\"), <br>\n",
    "    (1, \"b\"), <br>\n",
    "    (2, \"c\"), <br>\n",
    "    (3, \"a\"), <br>\n",
    "    (4, \"a\"), <br>\n",
    "    (5, \"c\")  <br>\n",
    "    ], [\"id\", \"category\"]) <br>\n",
    "<br>\n",
    "stringIndexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\") <br>\n",
    "model = stringIndexer.fit(df) <br>\n",
    "indexed = model.transform(df) <br>\n",
    "<br>\n",
    "encoder = OneHotEncoder(inputCol=\"categoryIndex\", outputCol=\"categoryVec\") <br>\n",
    "encoded = encoder.transform(indexed) <br>\n",
    "encoded.show()\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">Getting familiar with the SparkML Transformer: Bucketizer </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketizer transforms a column of continuous features to a column of feature buckets, where the buckets are specified by users. It takes a parameter defining the number of buckets. Bucketizing data is also referred to as \"binning\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel-group\" id=\"accordion-13\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-13\" href=\"#collapse1-13\">\n",
    "        Click on this link to expand this cell, then copy and paste the code which will appear in a new cell just below, and execute that cell to see how Bucketizer works. (You may subsequently delete that new cell and proceed with this notebook).</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-13\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "from pyspark.ml.feature import Bucketizer<br>\n",
    "<br>\n",
    "splits = [-float(\"inf\"), -0.5, 0.0, 0.5, float(\"inf\")] <br>\n",
    "<br>\n",
    "data = [(-999.9,), (-0.5,), (-0.3,), (0.0,), (0.2,), (999.9,)] <br>\n",
    "dataFrame = spark.createDataFrame(data, [\"features\"]) <br>\n",
    "<br>\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"features\", outputCol=\"bucketedFeatures\") <br>\n",
    "<br>\n",
    "# Transform original data into its bucket index. <br>\n",
    "bucketedData = bucketizer.transform(dataFrame) <br>\n",
    "<br>\n",
    "print(\"Bucketizer output with %d buckets\" % (len(bucketizer.getSplits())-1)) <br>\n",
    "bucketedData.show()\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">Getting familiar with the SparkML Transformer: VectorAssembler </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel-group\" id=\"accordion-14\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-14\" href=\"#collapse1-14\">\n",
    "        Click on this link to expand this cell, then copy and paste the code which will appear in a new cell just below, and execute that cell to see how VectorAssembler works. (You may subsequently delete that new cell and proceed with this notebook).</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-14\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "from pyspark.ml.linalg import Vectors <br>\n",
    "from pyspark.ml.feature import VectorAssembler <br>\n",
    "<br>\n",
    "dataset = spark.createDataFrame( <br>\n",
    "    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)], <br>\n",
    "    [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"]) <br>\n",
    "<br>\n",
    "assembler = VectorAssembler( <br>\n",
    "    inputCols=[\"hour\", \"mobile\", \"userFeatures\"], <br>\n",
    "    outputCol=\"features\") <br>\n",
    "<br>\n",
    "output = assembler.transform(dataset) <br>\n",
    "print(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\") <br>\n",
    "output.select(\"features\", \"clicked\").show(truncate=False) <br>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">Getting familiar with the SparkML Transformer: Normalizer </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizer is a Transformer which transforms a dataset of Vector rows, normalizing each Vector to have unit norm. It takes parameter p, which specifies the p-norm used for normalization. (p=2 by default.) This normalization can help standardize your input data and improve the behavior of learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel-group\" id=\"accordion-15\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-15\" href=\"#collapse1-15\">\n",
    "        Click on this link to expand this cell, then copy and paste the code which will appear in a new cell just below, and execute that cell to see how Normalizer works. (You may subsequently delete that new cell and proceed with this notebook).</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-15\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "from pyspark.ml.feature import Normalizer<br>\n",
    "from pyspark.ml.linalg import Vectors<br>\n",
    "<br>\n",
    "dataFrame = spark.createDataFrame([ <br>\n",
    "    (0, Vectors.dense([1.0, 0.5, -1.0]),), <br>\n",
    "    (1, Vectors.dense([2.0, 1.0, 1.0]),), <br>\n",
    "    (2, Vectors.dense([4.0, 10.0, 2.0]),) <br>\n",
    "], [\"id\", \"features\"]) <br>\n",
    "<br>\n",
    "# Normalize each Vector using $L^1$ norm.<br>\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)<br>\n",
    "l1NormData = normalizer.transform(dataFrame) <br>\n",
    "print(\"Normalized using L^1 norm\") <br>\n",
    "l1NormData.show() <br>\n",
    "<br>\n",
    "# Normalize each Vector using $L^\\infty$ norm. <br>\n",
    "lInfNormData = normalizer.transform(dataFrame, {normalizer.p: float(\"inf\")}) <br>\n",
    "print(\"Normalized using L^inf norm\") <br>\n",
    "lInfNormData.show()\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">There are several other Estimators and Transformers which are documented in the Apache documentation online at <span style=\"color:blue\">spark.apache.org </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#fa04d9\">Step 7: Applying the concepts described above to our customer churn dataset: \n",
    "** * a) Defining and applying the StringIndexer Estimator to input columns Gender, Status, CarOwner, Paymethod, LocalBilltype, LongDistanceBilltype. **<br>\n",
    "** * b) Defining and applying VectorAssembler to the columns above to group them as one input vector to the model. **<br>\n",
    "** * c) Defining and applying a StringIndexer Estimator to the target label column \"CHURN\", to encode the T/F values into 0/1. ** <br>\n",
    "** * d) Defining and applying an IndexToString Transformer to reverse the output of our model from 0/1 predictions back to T/F values . ** <br>\n",
    "** * e) Defining the Random Forest estimator itself, which will be trained on the input training data to produce the actual model which will perform the predictions. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a) Defining a StringIndexer for the String columns in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### In this dataset, we will encode columns Gender, Status, CarOwner, Paymethod, LocalBilltype and LongDistanceBilltype\n",
    "# StringIndexer encodes a string column of labels to a column of label indices. \n",
    "SI1 = StringIndexer(inputCol='Gender', outputCol='GenderEncoded')\n",
    "SI2 = StringIndexer(inputCol='Status',outputCol='StatusEncoded')\n",
    "SI3 = StringIndexer(inputCol='CarOwner',outputCol='CarOwnerEncoded')\n",
    "SI4 = StringIndexer(inputCol='Paymethod',outputCol='PaymethodEncoded')\n",
    "SI5 = StringIndexer(inputCol='LocalBilltype',outputCol='LocalBilltypeEncoded')\n",
    "SI6 = StringIndexer(inputCol='LongDistanceBilltype',outputCol='LongDistanceBilltypeEncoded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b) Define a Vector Assembler for all the columns of interest to be passed into the chosen machine learning model (columns which are encoded as well as those kept as is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pipelines API requires that input variables are passed in  a vector\n",
    "assembler = VectorAssembler(inputCols=[\"GenderEncoded\", \"StatusEncoded\", \"CarOwnerEncoded\", \"PaymethodEncoded\", \\\n",
    "                                       \"LocalBilltypeEncoded\", \"LongDistanceBilltypeEncoded\", \"Children\", \"EstIncome\", \"Age\", \\\n",
    "                                       \"LongDistance\", \"International\", \"Local\", \"Dropped\",\"Usage\",\"RatePlan\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;c) Defining a StringIndexer for the label column of our model (CHURN column. The values True and False will be converted to 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode the label column\n",
    "labelIndexer = StringIndexer(inputCol='CHURN', outputCol='label').fit(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;d) Defining an IndexToString transformer to bring the labels back to True and False once the predictions are done. The model will produce a column named \"prediction\" which will contain 0 or 1. We will convert it back to True and False in a column named \"predictedLabel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;e) Defining a Random Forest estimator. This is a popular tree based classifier method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiate the algorithm, take the default settings\n",
    "rf=RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#fa04d9\">Step 8: Defining a Spark ML pipeline:\n",
    "** * All the individual components of the pipeline have been defined in the section above. Notice how we will now \"group\" them into a pipeline object</span> **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### In machine learning, it is common to run a sequence of algorithms to process and learn from data. E.g., a simple text document processing workflow might include several stages:\n",
    "* Split each document’s text into words. \n",
    "* Convert each document’s words into a numerical feature vector.  \n",
    "* Learn a prediction model using the feature vectors and labels.<br>\n",
    "\n",
    "### MLlib represents such a workflow as a Pipeline, which consists of a sequence of PipelineStages (Transformers and Estimators) to be run in a specific order. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build the Spark pipeline including the operations defined in Step 7 above.\n",
    "\"Pipeline\" is an API in SparkML. A pipeline defines a sequence of transformers and estimators to perform the analysis in stages.\n",
    "Additional information on SparkML is available online, including at this link: https://spark.apache.org/docs/2.0.2/ml-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the pipeline\n",
    "pipeline = Pipeline(stages=[SI1,SI2,SI3,SI4,SI5,SI6, labelIndexer, assembler, rf, labelConverter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into Training and Testing sets (this is a standard best practice in data science)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test datasets\n",
    "(trainingData, testingData) = data_df.randomSplit([0.7, 0.3],seed=9)\n",
    "trainingData.cache()\n",
    "testingData.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model from fitting the whole pipeline using the training data set. <br><br>Note that the pipeline interface will correctly call fit+transform or just transform alone for each stage of the pipeline, depending on whether the current stage is an estimator (such as StringIndex) or a Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build model. The fitted model from a Pipeline is a PipelineModel, which consists of fitted models and transformers, corresponding to the pipeline stages.\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"color:#fa04d9\">Step 9: Score the test data set </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=model.transform(testingData)\n",
    "result_display=result.select(result[\"ID\"],result[\"CHURN\"],result[\"Label\"],result[\"predictedLabel\"],result[\"prediction\"],result[\"probability\"])\n",
    "result_display.toPandas().head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#fa04d9\">Step 10: Model Evaluation </span>\n",
    "** Find accuracy of the model and the Area Under the ROC Curve **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Model Accuracy = {:.2f}.'.format(result.filter(result.label == result.prediction).count() / float(result.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an evaluator for the binary classification using area under the ROC Curve as the evaluation metric\n",
    "Receiver operating characteristic (ROC) is a graphical plot that illustrates the performance of a binary classifier system as its discrimination threshold is varied.\n",
    "\n",
    "Additional reading on this metric can be found very easily online, such as at this wikipedia link: https://en.wikipedia.org/wiki/Receiver_operating_characteristic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "print 'Area under ROC curve = {:.2f}.'.format(evaluator.evaluate(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#fa04d9\">Step 11:  Tune the hyperparameters to find the best model </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Parameter Grid specifying the parameters to be evaluated to determine the best combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set different levels for the maxDepth\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "paramGrid = (ParamGridBuilder().addGrid(rf.maxDepth,[4,6,8]).build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a cross validator to tune the pipeline with the generated parameter grid\n",
    "Cross-validation attempts to fit the underlying estimator with user-specified combinations of parameters, cross-evaluate the fitted models, and output the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# perform 3 fold cross validation\n",
    "cv = CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "cvModel = cv.fit(trainingData)\n",
    "\n",
    "# pick the best model\n",
    "best_rfModel = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the test data set with the best model\n",
    "cvresult=best_rfModel.transform(testingData)\n",
    "cvresults_show=cvresult.select(cvresult[\"ID\"],cvresult[\"CHURN\"],cvresult[\"Label\"],cvresult[\"predictedLabel\"],cvresult[\"prediction\"],cvresult[\"probability\"])\n",
    "cvresults_show.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Model Accuracy of the best fitted model = {:.2f}.'.format(cvresult.filter(cvresult.label == cvresult.prediction).count()/ float(cvresult.count()))\n",
    "print 'Model Accuracy of the default model = {:.2f}.'.format(result.filter(result.label == result.prediction).count() / float(result.count()))\n",
    "print '   '\n",
    "print('Area under the ROC curve of best fitted model = {:.2f}.'.format(evaluator.evaluate(cvresult)))\n",
    "print 'Area under the ROC curve of the default model = {:.2f}.'.format(evaluator.evaluate(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#fa04d9\">Step 12: Execute an inline invocation of the best model which was just identified </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us now make a prediction on some customer for which we will provide our own made up attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gender = 'F'\n",
    "Status = 'M'\n",
    "CarOwner = 'N'\n",
    "Paymethod = 'CC'\n",
    "LocalBilltype = 'Budget'\n",
    "LongDistanceBilltype = 'Standard'\n",
    "Children = 1\n",
    "EstIncome = 45000\n",
    "Age = 30\n",
    "LongDistance = 30\n",
    "International = 0\n",
    "Local = 100\n",
    "Dropped = 0\n",
    "Usage = 150\n",
    "RatePlan = 2\n",
    "\n",
    "Features = (spark.createDataFrame([(Gender, Status, CarOwner, Paymethod, LocalBilltype, LongDistanceBilltype, Children, EstIncome, Age, LongDistance, \\\n",
    "                                              International, Local, Dropped, Usage, RatePlan)],\n",
    "    ['Gender', 'Status', 'CarOwner', 'Paymethod', 'LocalBilltype', 'LongDistanceBilltype', 'Children', 'EstIncome', 'Age', 'LongDistance', \\\n",
    "     'International', 'Local', 'Dropped', 'Usage', 'RatePlan']))\n",
    "Features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChurnPrediction = best_rfModel.transform(Features)\n",
    "ChurnPrediction.select('rawPrediction', 'probability', 'prediction').show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini Exercise: Change the number of children and/or the EstIncome in the cell prior to the one above, and observe the impact on the prediction:\n",
    "* It seems that a number of children lower than 3 will result in churn, but a customer with 3 children or more will not churn.\n",
    "* The rule above is true for lower incomes. With higher incomes, churn is less likely (if we change the income to 145,000 the model does not seem to predict churn anymore, regardless of the number of children)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#fa04d9\"> Step 13: Save Model (Not needed in this version of the notebook). </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">All the steps below are the optional exercises specific to this notebook for deepening your understanding of SparkML pipelines.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#fa04d9\">Step 14: Exercise 1: Injecting a new Transformer in your existing pipeline.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Tree classifiers (including the random forest used above) can handle categorical data directly in their input and do not require OneHot encoding (as it was described higher in this notebook in section 6). However, other classifiers such as logistic regression will typically not handle categorical data correctly (some wrong relationship may be inferred between the different encoded values) and do require OneHot encoding.\n",
    "\n",
    "In this exercise, it is suggested that you use the few blank cells below (and add new ones if needed) to modify the pipeline covered so far, by injecting OneHot encoding for the columns Gender, Status, CarOwner, Paymethod, LocalBilltype and LongDistanceBilltype. You can as well recopy the subsequent steps involving the evaluation of the model, following the examples provided higher in this notebook.\n",
    "\n",
    "Do you notice any difference in the accuracy of the model by injecting the OneHot encoding transformation in the pipeline ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#fa04d9\">Step 15: Exercise 2. Invoking new Estimators in your pipeline, besides the Random Forest used so far. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Go one step beyond the previous exercise, and change now the SparkML pipeline built so far to use different predictors, including Logistic Regression or Support Vector Machine (use the existing random forest predictor example as \"template\" and lookup examples for others on the apache spark website). \n",
    "\n",
    "Usually, logistic regression does not behave properly with large numbers of categorical variables and it is typically suggested to avoid categorical variables when using logistic regression. Consequently, you can reuse your OneHot encoding results from the previous exercise to feed OneHot encoded data into your model. Work as well on enhancing your pipeline by using some of the transformers described higher such as a normalizer, bucketizer or standardScaler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You have come to the end of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sidney Phoon**<br/>\n",
    "**Elena Lowery**<br/>\n",
    "**Rich Tarro**<br/>\n",
    "**Mokhtar Kandil**<br/>\n",
    "July, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.0",
   "language": "python",
   "name": "python2-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
